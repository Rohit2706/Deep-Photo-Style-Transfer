{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP PHOTO STYLE TRANSFER\n",
    "## PAPER CODE: 54\n",
    "\n",
    "#### GROUP MEMBERS\n",
    "\n",
    "2017A7PS0122P Rohit Jain\n",
    "\n",
    "2017A7PS0088P Vaishnavi Kotturu\n",
    "\n",
    "2017A3PS0267P Khushi Gupta\n",
    "\n",
    "(The code has been submitted in the partial fulfillment of the course Neural Networks and Fuzzy Logic 2019-20 Semester 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "vzBKB_zlFPYx",
    "outputId": "3890890d-1c50-46bb-e491-e83c09f56d84"
   },
   "outputs": [],
   "source": [
    "# Uncomment the following lines if using google colab\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header Files\n",
    "\n",
    "The code has following dependencies:\n",
    "1. torch\n",
    "2. matplotlib\n",
    "3. PIL\n",
    "4. torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9nZWc3KJ7igO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "TjotW_c1FSWs"
   },
   "outputs": [],
   "source": [
    "def tensor2img(x):\n",
    "\n",
    "    '''\n",
    "    Converts a torch tensor to an array\n",
    "    '''\n",
    "\n",
    "    x = x.detach().numpy()\n",
    "    x = x.transpose(0,2,3,1)\n",
    "    x = x.squeeze()\n",
    "    x = x.clip(0,1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def img2tensor(x):\n",
    "    \n",
    "    '''\n",
    "    Converts an image to a tensor\n",
    "    '''\n",
    "\n",
    "    if x.ndim == 2:\n",
    "        y=np.expand_dims(x, axis=0)\n",
    "        y=np.expand_dims(y, axis=0)\n",
    "        return torch.Tensor(y)\n",
    "    elif x.ndim == 3:\n",
    "        y=x.transpose(2, 0, 1)\n",
    "        y=np.expand_dims(y, axis=0)\n",
    "        return torch.Tensor(y)\n",
    "    elif x.ndim == 4:\n",
    "        y=x.transpose(0, 3, 1, 2)\n",
    "        return torch.Tensor(y)\n",
    "\n",
    "\n",
    "def extract_func(seg):\n",
    "\n",
    "    '''\n",
    "    Extracts the color masks from the segmented masks:\n",
    "    Blue, Green, Black, White, Red, Yellow, Grey, Light Blue, Purple\n",
    "    '''\n",
    "\n",
    "    ans = []\n",
    "\n",
    "    # BLUE\n",
    "    r = seg[..., 0] < 0.1\n",
    "    g = seg[..., 1] < 0.1\n",
    "    b = seg[..., 2] > 0.9\n",
    "    mask = r & g & b\n",
    "    ans.extend([mask])\n",
    "\n",
    "    # GREEN\n",
    "    r = seg[..., 0] < 0.1\n",
    "    g = seg[..., 1] > 0.9\n",
    "    b = seg[..., 2] < 0.1\n",
    "    mask = r & g & b\n",
    "    ans.extend([mask])\n",
    "\n",
    "    # BLACK\n",
    "    r = seg[..., 0] < 0.1\n",
    "    g = seg[..., 1] < 0.1\n",
    "    b = seg[..., 2] < 0.1\n",
    "    mask = r & g & b\n",
    "    ans.extend([mask])\n",
    "\n",
    "    # WHITE\n",
    "    r = seg[..., 0] > 0.9\n",
    "    g = seg[..., 1] > 0.9\n",
    "    b = seg[..., 2] > 0.9\n",
    "    mask = r & g & b\n",
    "    ans.extend([mask])\n",
    "\n",
    "    # RED\n",
    "    r = seg[..., 0] > 0.9\n",
    "    g = seg[..., 1] < 0.1\n",
    "    b = seg[..., 2] < 0.1\n",
    "    mask = r & g & b\n",
    "    ans.extend([mask])\n",
    "\n",
    "    # YELLOW\n",
    "    r = seg[..., 0] > 0.9\n",
    "    g = seg[..., 1] > 0.9\n",
    "    b = seg[..., 2] < 0.1\n",
    "    mask = r & g & b\n",
    "    ans.extend([mask])\n",
    "\n",
    "    # GREY\n",
    "    r = (seg[..., 0] > 0.4) & (seg[..., 0] < 0.6)\n",
    "    g = (seg[..., 1] > 0.4) & (seg[..., 1] < 0.6)\n",
    "    b = (seg[..., 2] > 0.4) & (seg[..., 2] < 0.6)\n",
    "    mask = r & g & b\n",
    "    ans.extend([mask])\n",
    "\n",
    "    # LIGHT BLUE\n",
    "    r = seg[..., 0] < 0.1\n",
    "    g = seg[..., 1] > 0.9\n",
    "    b = seg[..., 2] > 0.9\n",
    "    mask = r & g & b\n",
    "    ans.extend([mask])\n",
    "\n",
    "    # PURPLE\n",
    "    r = seg[..., 0] > 0.9\n",
    "    g = seg[..., 1] < 0.1\n",
    "    b = seg[..., 2] > 0.9\n",
    "    mask = r & g & b\n",
    "    ans.extend([mask])\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "def masks_func(img):\n",
    "    \n",
    "    '''\n",
    "    Returns the segmentation masks from the segmentated image.\n",
    "    '''\n",
    "\n",
    "    image = Image.open(img)\n",
    "    result = np.array(image, dtype=np.float) / 255\n",
    "    return extract_func(result)\n",
    "\n",
    "\n",
    "def get_masks(path1, path2):\n",
    "    \n",
    "    '''\n",
    "    Finds the dense masks to be considered for augmented style loss\n",
    "    '''\n",
    "\n",
    "    masks1 = masks_func(path1)\n",
    "    masks2 = masks_func(path2)\n",
    "\n",
    "    temp = []\n",
    "    for c, s in zip(masks2, masks1):\n",
    "        temp.append(np.mean(c) > 0.01 and np.mean(s) > 0.01)\n",
    "    \n",
    "    new1=[]\n",
    "    for i, j in zip(masks1, temp):\n",
    "        if j:\n",
    "            new1.append(i)\n",
    "        \n",
    "    new2=[]\n",
    "    for i, j in zip(masks2, temp):\n",
    "        if j:\n",
    "            new2.append(i)\n",
    "\n",
    "    return new1, new2\n",
    "\n",
    "\n",
    "def load_masks(path1, path2, size):\n",
    "    \n",
    "    '''\n",
    "    Loads the tensors for the mask images\n",
    "    '''\n",
    "\n",
    "    arr1, arr2 = get_masks(path1, path2)\n",
    "    resize_f = lambda x: resize(x, size, mode=\"reflect\")\n",
    "\n",
    "    arr1 = map(resize_f,arr1)\n",
    "    arr2 = map(resize_f,arr2)\n",
    "    arr1 = map(img2tensor,arr1)\n",
    "    arr2 = map(img2tensor,arr2)\n",
    "\n",
    "    return arr1, arr2\n",
    "\n",
    "def load_images(img_name, size):\n",
    "\n",
    "    '''\n",
    "    Loads images\n",
    "    '''\n",
    "\n",
    "    func = transforms.Compose([transforms.Resize(size), transforms.ToTensor()])\n",
    "    output = func(Image.open(img_name)).unsqueeze(0)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def plot_output(content, style, output):\n",
    "    \n",
    "    '''\n",
    "    Utility function to plot the content, style and output image\n",
    "    '''\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(tensor2img(style))\n",
    "    plt.title(\"Style Image\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(tensor2img(output))\n",
    "    plt.title(\"Output Image\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(tensor2img(content))\n",
    "    plt.title(\"Content Image\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Photo Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rJcL_SXiFYdJ"
   },
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    Normalizes the input image to be put in our model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "\n",
    "class Content_loss(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    Calculates the content variation between the content image and the output image at the given layer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, target):\n",
    "        super(Content_loss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "\n",
    "def gram_matrix(input_image):\n",
    "\n",
    "    '''\n",
    "    Measures the style in the input image by calculating the correlation between its channels\n",
    "    '''\n",
    "    \n",
    "    batch_size, number_of_features, feature_dim1, feature_dim2 = input_image.size() \n",
    "    features = input_image.view(batch_size * number_of_features, feature_dim1 * feature_dim2)\n",
    "    G = torch.mm(features, features.t())\n",
    "    G = G.div(batch_size * number_of_features* feature_dim1 * feature_dim2)\n",
    "    \n",
    "    return G\n",
    "    \n",
    "    \n",
    "\n",
    "class augmented_style_loss(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Exploits the semantic information of the style image  and calculates the style variation segment wise rather than calculating on the entire image at once \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_feature, target_masks, input_masks):\n",
    "        super(augmented_style_loss, self).__init__()\n",
    "        self.input_masks = [mask.detach() for mask in input_masks]\n",
    "        self.targets = [\n",
    "            gram_matrix(target_feature * mask).detach() for mask in target_masks\n",
    "        ]\n",
    "\n",
    "    def forward(self, input):\n",
    "        gram_matrices = [\n",
    "            gram_matrix(input * mask.detach()) for mask in self.input_masks\n",
    "        ]\n",
    "        self.loss = sum(\n",
    "            F.mse_loss(gram, target)\n",
    "            for gram, target in zip(gram_matrices, self.targets)\n",
    "        )\n",
    "        return input\n",
    "\n",
    "\n",
    "def model_and_losses(vgg, mean, std, content_image, style_image, content_layers, style_layers, content_masks, style_masks):\n",
    "\n",
    "    '''\n",
    "    Constructs and returns the models and losses while stylizing the content image\n",
    "    '''\n",
    "    \n",
    "    vgg = copy.deepcopy(vgg)\n",
    "    normalization = Normalization(mean, std)\n",
    "\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    num_pool_layers = 0 \n",
    "    num_conv_layers = 0\n",
    "    \n",
    "    for model_layer in vgg.children():\n",
    "        if isinstance(model_layer, nn.Conv2d):\n",
    "            num_conv_layers += 1\n",
    "            name = \"conv{}_{}\".format(num_pool_layers, num_conv_layers)\n",
    "\n",
    "        elif isinstance(model_layer, nn.ReLU):\n",
    "            name = \"relu{}_{}\".format(num_pool_layers, num_conv_layers)\n",
    "            model_layer = nn.ReLU(inplace=False)\n",
    "\n",
    "        elif isinstance(model_layer, nn.MaxPool2d):\n",
    "            num_pool_layers += 1\n",
    "            num_conv_layers = 0\n",
    "            name = \"pool_{}\".format(num_pool_layers)\n",
    "            model_layer = nn.AvgPool2d(kernel_size=model_layer.kernel_size, stride=model_layer.stride, padding=model_layer.padding)\n",
    "            style_masks = [model_layer(mask) for mask in style_masks]\n",
    "            content_masks = [model_layer(mask) for mask in content_masks]\n",
    "\n",
    "        elif isinstance(model_layer, nn.BatchNorm2d):\n",
    "            name = \"bn{}_{}\".format(num_pool_layers, num_conv_layers)\n",
    "\n",
    "        model.add_module(name, model_layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_image).detach()\n",
    "            content_loss = Content_loss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(num_pool_layers), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_image).detach()\n",
    "\n",
    "            style_loss = augmented_style_loss(target_feature, style_masks, content_masks)\n",
    "            model.add_module(\"style_loss_{}\".format(num_pool_layers), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    return model, content_losses, style_losses\n",
    "\n",
    "\n",
    "def get_input_optimizer(input_image):\n",
    "\n",
    "    '''\n",
    "    Returns the LBFGS optimizer to be used for backpropagation.\n",
    "    '''\n",
    "    \n",
    "    optimizer = optim.LBFGS([input_image.requires_grad_()])\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def photoreg(input_image, L):\n",
    "    \n",
    "    '''\n",
    "    Calculates the the losses and gradients for photorealism regularization\n",
    "    '''\n",
    "    \n",
    "    image = tensor2img(input_image)\n",
    "    gradient = L.dot(image.reshape(-1, 3))\n",
    "    loss = (gradient * image.reshape(-1, 3))\n",
    "    gradient = gradient.reshape(image.shape)\n",
    "    loss = loss.sum()\n",
    "    \n",
    "    return loss, 2.0 * gradient\n",
    "\n",
    "\n",
    "def stylize(vgg, mean, std, content_image, style_image, output_image, content_layers, style_layers, content_masks, style_masks, num_steps=300, style_weight=100000, content_weight=1000, reg_weight=1000):\n",
    "\n",
    "    \"\"\"\n",
    "    Runs the style transfer by using:\n",
    "    1. Matting Laplacian to locally affine in color space\n",
    "    2. Semantic Segmentation for augmented style loss\n",
    "    3. Photorealism Regularization to make photos look more natural\n",
    "    \"\"\"\n",
    "    \n",
    "    model, content_losses, style_losses = model_and_losses(vgg, mean, std, content_image, style_image, content_layers, style_layers, content_masks, style_masks)\n",
    "    optimizer = get_input_optimizer(output_image)\n",
    "    L = compute_laplacian(tensor2img(content_image))\n",
    "\n",
    "    step = 0\n",
    "    while step <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            output_image.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(output_image)\n",
    "\n",
    "            get_loss = lambda x: x.loss\n",
    "            style_score = style_weight * sum(map(get_loss, style_losses))\n",
    "            content_score = content_weight * sum(map(get_loss, content_losses))\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            reg_loss, reg_grad = photoreg(output_image, L)\n",
    "            reg_grad_tensor = img2tensor(reg_grad)\n",
    "\n",
    "            output_image.grad += reg_weight * reg_grad_tensor\n",
    "\n",
    "            loss += reg_weight * reg_loss\n",
    "\n",
    "            nonlocal step\n",
    "            step += 1\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                print(\"iteration {:>4d}:\".format(step), \"Style Loss = {:.3f} Content Loss = : {:.3f} Reg Loss = :{:.3f}\".format(style_score.item(), content_score.item(), reg_loss))\n",
    "\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    output_image.data.clamp_(0, 1)\n",
    "\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matting Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fnr9CtbDFa0K"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import logging\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "\n",
    "def _rolling_block(A, block=(3, 3)):\n",
    "    \"\"\"Applies sliding window to given matrix.\"\"\"\n",
    "    shape = (A.shape[0] - block[0] + 1, A.shape[1] - block[1] + 1) + block\n",
    "    strides = (A.strides[0], A.strides[1]) + A.strides\n",
    "    return as_strided(A, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "def compute_laplacian(img, mask=None, eps=10 ** (-7), win_rad=1):\n",
    "    \"\"\"Computes Matting Laplacian for a given image.\n",
    "    Args:\n",
    "        img: 3-dim numpy matrix with input image\n",
    "        mask: mask of pixels for which Laplacian will be computed.\n",
    "            If not set Laplacian will be computed for all pixels.\n",
    "        eps: regularization parameter controlling alpha smoothness\n",
    "            from Eq. 12 of the original paper. Defaults to 1e-7.\n",
    "        win_rad: radius of window used to build Matting Laplacian (i.e.\n",
    "            radius of omega_k in Eq. 12).\n",
    "    Returns: sparse matrix holding Matting Laplacian.\n",
    "    \"\"\"\n",
    "\n",
    "    win_size = (win_rad * 2 + 1) ** 2\n",
    "    h, w, d = img.shape\n",
    "    # Number of window centre indices in h, w axes\n",
    "    c_h, c_w = h - 2 * win_rad, w - 2 * win_rad\n",
    "    win_diam = win_rad * 2 + 1\n",
    "\n",
    "    indsM = np.arange(h * w).reshape((h, w))\n",
    "    ravelImg = img.reshape(h * w, d)\n",
    "    win_inds = _rolling_block(indsM, block=(win_diam, win_diam))\n",
    "\n",
    "    win_inds = win_inds.reshape(c_h, c_w, win_size)\n",
    "    if mask is not None:\n",
    "        mask = cv2.dilate(\n",
    "            mask.astype(np.uint8), np.ones((win_diam, win_diam), np.uint8)\n",
    "        ).astype(np.bool)\n",
    "        win_mask = np.sum(mask.ravel()[win_inds], axis=2)\n",
    "        win_inds = win_inds[win_mask > 0, :]\n",
    "    else:\n",
    "        win_inds = win_inds.reshape(-1, win_size)\n",
    "\n",
    "    winI = ravelImg[win_inds]\n",
    "\n",
    "    win_mu = np.mean(winI, axis=1, keepdims=True)\n",
    "    win_var = np.einsum(\"...ji,...jk ->...ik\", winI, winI) / win_size - np.einsum(\n",
    "        \"...ji,...jk ->...ik\", win_mu, win_mu\n",
    "    )\n",
    "\n",
    "    inv = np.linalg.inv(win_var + (eps / win_size) * np.eye(3))\n",
    "\n",
    "    X = np.einsum(\"...ij,...jk->...ik\", winI - win_mu, inv)\n",
    "    vals = np.eye(win_size) - (1.0 / win_size) * (\n",
    "        1 + np.einsum(\"...ij,...kj->...ik\", X, winI - win_mu)\n",
    "    )\n",
    "\n",
    "    nz_indsCol = np.tile(win_inds, win_size).ravel()\n",
    "    nz_indsRow = np.repeat(win_inds, win_size).ravel()\n",
    "    nz_indsVal = vals.ravel()\n",
    "    L = scipy.sparse.coo_matrix(\n",
    "        (nz_indsVal, (nz_indsRow, nz_indsCol)), shape=(h * w, h * w)\n",
    "    )\n",
    "    return L\n",
    "\n",
    "\n",
    "def closed_form_matting_with_prior(image, prior, prior_confidence, consts_map=None):\n",
    "    \"\"\"Applies closed form matting with prior alpha map to image.\n",
    "    Args:\n",
    "        image: 3-dim numpy matrix with input image.\n",
    "        prior: matrix of same width and height as input image holding apriori alpha map.\n",
    "        prior_confidence: matrix of the same shape as prior hodling confidence of prior alpha.\n",
    "        consts_map: binary mask of pixels that aren't expected to change due to high\n",
    "            prior confidence.\n",
    "    Returns: 2-dim matrix holding computed alpha map.\n",
    "    \"\"\"\n",
    "\n",
    "    assert image.shape[:2] == prior.shape, (\n",
    "        \"prior must be 2D matrix with height and width equal \" \"to image.\"\n",
    "    )\n",
    "    assert image.shape[:2] == prior_confidence.shape, (\n",
    "        \"prior_confidence must be 2D matrix with \" \"height and width equal to image.\"\n",
    "    )\n",
    "    assert (consts_map is not None) or image.shape[\n",
    "        :2\n",
    "    ] == consts_map.shape, (\n",
    "        \"consts_map must be 2D matrix with height and width equal to image.\"\n",
    "    )\n",
    "\n",
    "    logging.info(\"Computing Matting Laplacian.\")\n",
    "    laplacian = compute_laplacian(\n",
    "        image, ~consts_map if consts_map is not None else None\n",
    "    )\n",
    "\n",
    "    confidence = scipy.sparse.diags(prior_confidence.ravel())\n",
    "    logging.info(\"Solving for alpha.\")\n",
    "    solution = scipy.sparse.linalg.spsolve(\n",
    "        laplacian + confidence, prior.ravel() * prior_confidence.ravel()\n",
    "    )\n",
    "    alpha = np.minimum(np.maximum(solution.reshape(prior.shape), 0), 1)\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def closed_form_matting_with_trimap(image, trimap, trimap_confidence=100.0):\n",
    "    \"\"\"Apply Closed-Form matting to given image using trimap.\"\"\"\n",
    "\n",
    "    assert image.shape[:2] == trimap.shape, (\n",
    "        \"trimap must be 2D matrix with height and width equal \" \"to image.\"\n",
    "    )\n",
    "    consts_map = (trimap < 0.1) | (trimap > 0.9)\n",
    "    return closed_form_matting_with_prior(\n",
    "        image, trimap, trimap_confidence * consts_map, consts_map\n",
    "    )\n",
    "\n",
    "\n",
    "def closed_form_matting_with_scribbles(image, scribbles, scribbles_confidence=100.0):\n",
    "    \"\"\"Apply Closed-Form matting to given image using scribbles image.\"\"\"\n",
    "\n",
    "    assert (\n",
    "        image.shape == scribbles.shape\n",
    "    ), \"scribbles must have exactly same shape as image.\"\n",
    "    consts_map = np.sum(abs(image - scribbles), axis=-1) > 0.001\n",
    "    return closed_form_matting_with_prior(\n",
    "        image, scribbles[:, :, 0], scribbles_confidence * consts_map, consts_map\n",
    "    )\n",
    "\n",
    "\n",
    "closed_form_matting = closed_form_matting_with_trimap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code in Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "colab_type": "code",
    "id": "3tj6sc-jFnjf",
    "outputId": "73326179-0423-4361-a3ec-9ee4f6df008a"
   },
   "outputs": [],
   "source": [
    "imsize = (128,128)\n",
    "\n",
    "# Please pass the image file path in the corresponding fields\n",
    "\n",
    "style_image = load_images(\"tar16.png\", imsize)\n",
    "content_image = load_images(\"in16.png\", imsize)\n",
    "output_image = content_image.clone()\n",
    "\n",
    "style_masks, content_masks = load_masks(\"segtar16.png\",\"segin16.png\",imsize)\n",
    "\n",
    "vgg = models.vgg19(pretrained=True).features.eval()\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "style_layers = [\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\"]\n",
    "content_layers = [\"conv4_2\"]\n",
    "\n",
    "output = stylize(vgg, mean, std, content_image, style_image, output_image, content_layers, style_layers, content_masks, style_masks,style_weight=1e6,\n",
    "    content_weight=1e4,\n",
    "    reg_weight=1e-4,\n",
    "    num_steps=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJ-0znniF4KZ"
   },
   "outputs": [],
   "source": [
    "plot_output(content_image, style_image, output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_code_to_be_submitted.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
